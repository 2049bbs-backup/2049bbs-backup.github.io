---
aid: 1166
cid: 16
authorID: 1227
addTime: 2019-05-09T13:42:00.000Z
title: 今天又被知乎恶心到了，然后有个想法
tags:
    - 想法
comments: []
date: 2019-05-09T13:42:00.000Z
category: '2049'
---

关于定时爬取知乎热门社会性事件问题下的内容

经常发生社会性新闻后，知乎上的东西就会被删掉。关键是被删的回答和问题总是很“精华”的。 因此我在想，既然全站爬取知乎内容比较难，为何不搞一个项目专门在热点事件发生时，第一时间让爬虫去监控对应的知乎问题下的所有内容呢？

然后把数据公开。比如利用Github API存到Github上。当然，提供良好的前端页面的话，就更好了。

类似这个项目：[http://206.189.252.32:3838/Wechatscope/](http://206.189.252.32:3838/Wechatscope/) [http://206.189.252.32:3838/](http://206.189.252.32:3838/)

这个项目还算比较有意义吧。怎么样，有没有技术大佬搞的？ 我也把这个想法发到其他地方，比如联系下品葱的那群大佬。
